worker()
list()
source('~/GitHub/cs-x-programming/week5/week5_tdm.R')
setwd("~/GitHub/cs-x-programming/week5")
setwd("~/")
setwd("~/GitHub/cs-x-programming/week5")
source('~/GitHub/cs-x-programming/week5/week5_tdm.R')
Allfreq
docnum
docs
page
mixseg
Alltoken
Allfreq
tokem
token
NewDataFrame
mixseg
freqFrame
DF
source('~/GitHub/cs-x-programming/week5/week5_tfidf.R')
tdm$nrow
as.character(id)
data
getdoc
library(bitops)
library(httr)
library(RCurl)
library(XML)
library(tm)
library(NLP)
library(tmcn)
library(jiebaRD)
library(jiebaR)
rm(list = ls())
library(rvest)
from <- 3870 # 2018-03-25
to   <- 3874 # 2018-03-31
prefix = "https://www.ptt.cc/bbs/NBA/index"
data <- list()
for( id in c(from:to) )
{
url  <- paste0( prefix, as.character(id), ".html" )
html <- htmlParse( GET(url) )
url.list <- xpathSApply( html, "//div[@class='title']/a[@href]", xmlAttrs )
data <- rbind( data, as.matrix(paste('https://www.ptt.cc', url.list, sep='')) )
res <- read_html(url)
raw.titles <- res %>% html_nodes("div.title")
nba.article.title <- raw.titles %>% html_text()
}
#I learned the difference between paste() and paste0(), the former can give order to seperate different things in (), and the latter will connect things in () without any blankspace
data <- unlist(data)
#take a loot at the data I successfully got
head(data)
tail(data)
data
nba.article.title
#Fail
#I want to get the titles of all the articles, but the result is only the last twenty titles.
library(dplyr)
getdoc <- function(url)
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
doc <- unlist(doc)
doc
library(bitops)
library(httr)
library(RCurl)
library(XML)
library(tm)
library(NLP)
library(tmcn)
library(jiebaRD)
library(jiebaR)
rm(list = ls())
library(rvest)
from <- 3870 # 2018-03-25
to   <- 3874 # 2018-03-31
prefix = "https://www.ptt.cc/bbs/NBA/index"
data <- list()
for( id in c(from:to) )
{
url  <- paste0( prefix, as.character(id), ".html" )
html <- htmlParse( GET(url) )
url.list <- xpathSApply( html, "//div[@class='title']/a[@href]", xmlAttrs )
data <- rbind( data, as.matrix(paste('https://www.ptt.cc', url.list, sep='')) )
res <- read_html(url)
raw.titles <- res %>% html_nodes("div.title")
nba.article.title <- raw.titles %>% html_text()
}
#I learned the difference between paste() and paste0(), the former can give order to seperate different things in (), and the latter will connect things in () without any blankspace
data <- unlist(data)
#take a loot at the data I successfully got
head(data)
tail(data)
data
nba.article.title
#Fail
#I want to get the titles of all the articles, but the result is only the last twenty titles.
library(dplyr)
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
}
sapply(data, getdoc)
#doc  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
}
library(dplyr)
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
#doc  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
}
sapply(data, getdoc)
library(dplyr)
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
doc111  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
}
sapply(data, getdoc)
#Success
#Got all the content,
library(dplyr)
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
doc111  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
}
sapply(data, getdoc)
#Success
#Got
library(dplyr)
getdoc111 <- function(url)
{
html <- htmlParse( getURL(url) )
doc111  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
}
sapply(data, getdoc)
#Success
library(dplyr)
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
}
sapply(data, getdoc)
setwd("~/GitHub/cs-x-programming/week6")
setwd("~/GitHub/cs-x-programming/week5")
setwd("~/GitHub/cs-x-programming/week6")
setwd("~/GitHub/cs-x-programming/week6")
setwd("~/GitHub/cs-x-programming/week6")
library(bitops)
library(httr)
library(RCurl)
library(XML)
library(tm)
library(NLP)
library(tmcn)
library(jiebaRD)
library(jiebaR)
library(readtext)
rm(list = ls())
library(rvest)
#I aim to analyze the keywords of NBA articles in PTT. Hopefully, the result can show me whether there is a team that is relatively popular in discussion. Also, I want to know what is the direction and keywords in these articles.
#Attain the website of the articles
from <- 3870 # 2018-03-25
to   <- 3874 # 2018-03-31
prefix = "https://www.ptt.cc/bbs/NBA/index"
data <- list()
for( id in c(from:to) )
{
url  <- paste0( prefix, as.character(id), ".html" )
html <- htmlParse( GET(url) )
url.list <- xpathSApply( html, "//div[@class='title']/a[@href]", xmlAttrs )
data <- rbind( data, as.matrix(paste('https://www.ptt.cc', url.list, sep='')) )
#print(paste(id))
#add this line so that I know the for loop has run successfully.
res <- read_html(url)
#print(paste(url))
#by adding this line, I understood the reason why I only output part of the titles from the website.
raw.titles <- res %>% html_nodes("div.title")
nba.article.title <- raw.titles %>% html_text()
}
#I learned the difference between paste() and paste0(), the former can give order to seperate different things in (), and the latter will connect things in () without any blankspace
data <- unlist(data)
#take a loot at the data I successfully got
head(data)
tail(data)
data
nba.article.title
#Success
#Attain all the websites.
#Fail
#I want to get the titles of all the articles, but the result is only the last twenty titles.
#Since extracting titles is not the main mission of this project, I focus on the rest of the work.
#Attain the contents of the artickles I look for.
#The difference between this project and the previous assignments is that I want to know the freqeucy of each keywords by time.
#At first, I dealed with extracting all the contents from websites(this part was finished first), and then focuss on sorting the content with hours.And then, I failed. There were lots of errors I couldn't solve, so I put it into the same{}.
#Problem: Perhaps I am processing a lot of data, RStudio isn't functioning well. It took a long time to show me the result after running the programs, which took me more time to work on it.
library(dplyr)
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
}
sapply(data, getdoc)
#Success
#Got all the content, and I also learned the differences between apply, sapply, lapply.
#I am confused how doc works.I understand that I use xpathsApply to gain content, but if I change the name of doc to doc111, it doesnt work.
#Confused, I don't know why there is a lot of blank space in my html.
#Fail
#I couldn't sort the articles in different timing classification.
#In order to finish this project, I simplified my work without the time factor.
page <- readtext("*.txt", encoding = "big5")
docs <- Corpus(VectorSource(page$text))
docnum = length(page)
mixseg = worker()
Alltoken = list()
Allfreq = list()
#listed all the words and phrases
for( c in 1:docnum )
{
token = list(jiebaR::segment(docs[[c]]$content, mixseg))
Alltoken = append(Alltoken, token)
#use append function to get every words and phrases I need
freq = list(as.data.frame(table(token)))
Allfreq = append(Allfreq, freq)
#use append function to get freqeuncy
}
docnum
docs$`1`
page$doc_id
page$text
setwd("~/GitHub/cs-x-programming/week6")
setwd("~/GitHub/cs-x-programming/week6")
setwd("~/GitHub/cs-x-programming/week6")
source('~/GitHub/cs-x-programming/week6/project1.R')
source('~/GitHub/cs-x-programming/week6/project1.R')
{
token = list(jiebaR::segment(docs[[c]]$content, mixseg))
Alltoken = append(Alltoken, token)
#use append function to get every words and phrases I need
freq = list(as.data.frame(table(token)))
Allfreq = append(Allfreq, freq)
#use append function to get freqeuncy
}
NewDataFrame
token
write.csv(NewDataFrame)
setwd("~/GitHub/cs-x-programming/week6")
setwd("~/GitHub/cs-x-programming/week6")
setwd("~/GitHub/cs-x-programming/week6")
write.csv(tfidfnn, "show.csv")
source('~/GitHub/cs-x-programming/week6/project1_tfidf.R')
source('~/GitHub/cs-x-programming/week6/project1_tfidf.R')
# cut the words
keywords = read.csv("keywords.csv")
print(paste(tfidfnn))
tfidfnn
print(paste(tfidfnn$Terms))
tfidfnn[1]
install.packages("varhandle")
source('~/GitHub/cs-x-programming/week6/project1_tfidf.R')
