library(bitops)
library(httr)
library(RCurl)
library(XML)
library(tm)
library(NLP)
library(tmcn)
library(jiebaRD)
library(jiebaR)
rm(list = ls())
library(rvest)
#I aim to analyze the keywords of NBA articles in PTT. Hopefully, the result can show me whether there is a team that is relatively popular in discussion. Also, I want to know what is the direction and keywords in these articles.
#Attain the website of the articles
from <- 3868 # 2018-03-29
to   <- 3876 # 2018-04-01
prefix = "https://www.ptt.cc/bbs/NBA/index"
data <- list()
for( id in c(from:to) )
{
url  <- paste0( prefix, as.character(id), ".html" )
html <- htmlParse( GET(url) )
url.list <- xpathSApply( html, "//div[@class='title']/a[@href]", xmlAttrs )
data <- rbind( data, as.matrix(paste('https://www.ptt.cc', url.list, sep='')) )
#print(paste(id))
#add this line so that I know the for loop has run successfully.
res <- read_html(url)
#print(paste(url))
#by adding this line, I understood the reason why I only output part of the titles from the website.
raw.titles <- res %>% html_nodes("div.title")
nba.article.title <- raw.titles %>% html_text()
}
#I learned the difference between paste() and paste0(), the former can give order to seperate different things in (), and the latter will connect things in () without any blankspace
data <- unlist(data)
#take a loot at the data I successfully got
head(data)
tail(data)
#data
#nba.article.title
#Success
#Attain all the websites.
#Fail
#I want to get the titles of all the articles, but the result is only the last twenty titles.
#Since extracting titles is not the main mission of this project, I focus on the rest of the work.
#Attain the contents of the artickles I look for.
#The difference between this project and the previous assignments is that I want to know the freqeucy of each keywords by time.
#At first, I dealed with extracting all the contents from websites(this part was finished first), and then focuss on sorting the content with hours.And then, I failed. There were lots of errors I couldn't solve, so I put it into the same{}.
#Problem: Perhaps I am processing a lot of data, RStudio isn't functioning well. It took a long time to show me the result after running the programs, which took me more time to work on it.
library(dplyr)
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
time <- xpathSApply( html, "//*[@id='main-content']/div[4]/span[2]", xmlValue )
temp.time <- gsub( "  ", " 0", unlist(time) )
part <- strsplit( temp.time, split=" ", fixed=TRUE )
timestamp <- part[[1]][4]
timestamp <- strsplit( timestamp, split=":", fixed=TRUE )
hour <- timestamp[[1]][1]
print(hour)
name <- paste0('./DATA/', hour, ".txt")
write(doc, name, append = TRUE)
}
sapply(data, getdoc)
library(bitops)
library(httr)
library(RCurl)
library(XML)
library(tm)
library(NLP)
library(tmcn)
library(jiebaRD)
library(jiebaR)
rm(list = ls())
library(rvest)
#I aim to analyze the keywords of NBA articles in PTT. Hopefully, the result can show me whether there is a team that is relatively popular in discussion. Also, I want to know what is the direction and keywords in these articles.
#Attain the website of the articles
from <- 3868 # 2018-03-29
to   <- 3876 # 2018-04-01
prefix = "https://www.ptt.cc/bbs/NBA/index"
data <- list()
for( id in c(from:to) )
{
url  <- paste0( prefix, as.character(id), ".html" )
html <- htmlParse( GET(url) )
url.list <- xpathSApply( html, "//div[@class='title']/a[@href]", xmlAttrs )
data <- rbind( data, as.matrix(paste('https://www.ptt.cc', url.list, sep='')) )
#print(paste(id))
#add this line so that I know the for loop has run successfully.
res <- read_html(url)
#print(paste(url))
#by adding this line, I understood the reason why I only output part of the titles from the website.
raw.titles <- res %>% html_nodes("div.title")
nba.article.title <- raw.titles %>% html_text()
}
#I learned the difference between paste() and paste0(), the former can give order to seperate different things in (), and the latter will connect things in () without any blankspace
data <- unlist(data)
#take a loot at the data I successfully got
head(data)
tail(data)
#data
#nba.article.title
#Success
#Attain all the websites.
#Fail
#I want to get the titles of all the articles, but the result is only the last twenty titles.
#Since extracting titles is not the main mission of this project, I focus on the rest of the work.
#Attain the contents of the artickles I look for.
#The difference between this project and the previous assignments is that I want to know the freqeucy of each keywords by time.
#At first, I dealed with extracting all the contents from websites(this part was finished first), and then focuss on sorting the content with hours.And then, I failed. There were lots of errors I couldn't solve, so I put it into the same{}.
#Problem: Perhaps I am processing a lot of data, RStudio isn't functioning well. It took a long time to show me the result after running the programs, which took me more time to work on it.
library(dplyr)
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
time <- xpathSApply( html, "//*[@id='main-content']/div[4]/span[2]", xmlValue )
temp.time <- gsub( "  ", " 0", unlist(time) )
part <- strsplit( temp.time, split=" ", fixed=TRUE )
timestamp <- part[[1]][4]
timestamp <- strsplit( timestamp, split=":", fixed=TRUE )
hour <- timestamp[[1]][1]
print(hour)
print(temp.time)
name <- paste0('./DATA/', hour, ".txt")
write(doc, name, append = TRUE)
}
sapply(data, getdoc)
{
doc.tfidf[x,y] <- (doc.tfidf[x,y] / tf[y]) * idf[x]
}
source('~/GitHub/cs-x-programming/week5/week5_tfidf.R')
setwd("~/GitHub/cs-x-programming/week6")
setwd("~/GitHub/cs-x-programming/week6")
setwd("~/GitHub/cs-x-programming/week6")
library(bitops)
library(httr)
library(RCurl)
library(XML)
library(tm)
library(NLP)
library(tmcn)
library(jiebaRD)
library(jiebaR)
rm(list = ls())
library(rvest)
#I aim to analyze the keywords of NBA articles in PTT. Hopefully, the result can show me whether there is a team that is relatively popular in discussion. Also, I want to know what is the direction and keywords in these articles.
#Attain the website of the articles
from <- 3868 # 2018-03-29
to   <- 3876 # 2018-04-01
prefix = "https://www.ptt.cc/bbs/NBA/index"
data <- list()
for( id in c(from:to) )
{
url  <- paste0( prefix, as.character(id), ".html" )
html <- htmlParse( GET(url) )
url.list <- xpathSApply( html, "//div[@class='title']/a[@href]", xmlAttrs )
data <- rbind( data, as.matrix(paste('https://www.ptt.cc', url.list, sep='')) )
#print(paste(id))
#add this line so that I know the for loop has run successfully.
res <- read_html(url)
#print(paste(url))
#by adding this line, I understood the reason why I only output part of the titles from the website.
raw.titles <- res %>% html_nodes("div.title")
nba.article.title <- raw.titles %>% html_text()
}
#I learned the difference between paste() and paste0(), the former can give order to seperate different things in (), and the latter will connect things in () without any blankspace
data <- unlist(data)
#take a loot at the data I successfully got
head(data)
tail(data)
#data
#nba.article.title
#Success
#Attain all the websites.
#Fail
#I want to get the titles of all the articles, but the result is only the last twenty titles.
#Since extracting titles is not the main mission of this project, I focus on the rest of the work.
#Attain the contents of the artickles I look for.
#The difference between this project and the previous assignments is that I want to know the freqeucy of each keywords by time.
#At first, I dealed with extracting all the contents from websites(this part was finished first), and then focuss on sorting the content with hours.And then, I failed. There were lots of errors I couldn't solve, so I put it into the same{}.
#Problem: Perhaps I am processing a lot of data, RStudio isn't functioning well. It took a long time to show me the result after running the programs, which took me more time to work on it.
library(dplyr)
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
time <- xpathSApply( html, "//*[@id='main-content']/div[4]/span[2]", xmlValue )
temp.time <- gsub( "  ", " 0", unlist(time) )
part <- strsplit( temp.time, split=" ", fixed=TRUE )
timestamp <- part[[1]][4]
timestamp <- strsplit( timestamp, split=":", fixed=TRUE )
hour <- timestamp[[1]][1]
print(hour)
print(temp.time)
name <- paste0('./DATA/', hour, ".txt")
write(doc, name, append = TRUE)
}
sapply(data, getdoc)
library(bitops)
library(httr)
library(RCurl)
library(XML)
library(tm)
library(NLP)
library(tmcn)
library(jiebaRD)
library(jiebaR)
rm(list = ls())
library(rvest)
#I aim to analyze the keywords of NBA articles in PTT. Hopefully, the result can show me whether there is a team that is relatively popular in discussion. Also, I want to know what is the direction and keywords in these articles.
#Attain the website of the articles
from <- 3868 # 2018-03-29
to   <- 3876 # 2018-04-01
prefix = "https://www.ptt.cc/bbs/NBA/index"
data <- list()
for( id in c(from:to) )
{
url  <- paste0( prefix, as.character(id), ".html" )
html <- htmlParse( GET(url) )
url.list <- xpathSApply( html, "//div[@class='title']/a[@href]", xmlAttrs )
data <- rbind( data, as.matrix(paste('https://www.ptt.cc', url.list, sep='')) )
#print(paste(id))
#add this line so that I know the for loop has run successfully.
res <- read_html(url)
#print(paste(url))
#by adding this line, I understood the reason why I only output part of the titles from the website.
raw.titles <- res %>% html_nodes("div.title")
nba.article.title <- raw.titles %>% html_text()
}
#I learned the difference between paste() and paste0(), the former can give order to seperate different things in (), and the latter will connect things in () without any blankspace
data <- unlist(data)
#take a loot at the data I successfully got
head(data)
tail(data)
#data
#nba.article.title
#Success
#Attain all the websites.
#Fail
#I want to get the titles of all the articles, but the result is only the last twenty titles.
#Since extracting titles is not the main mission of this project, I focus on the rest of the work.
#Attain the contents of the artickles I look for.
#The difference between this project and the previous assignments is that I want to know the freqeucy of each keywords by time.
#At first, I dealed with extracting all the contents from websites(this part was finished first), and then focuss on sorting the content with hours.And then, I failed. There were lots of errors I couldn't solve, so I put it into the same{}.
#Problem: Perhaps I am processing a lot of data, RStudio isn't functioning well. It took a long time to show me the result after running the programs, which took me more time to work on it.
library(dplyr)
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
time <- xpathSApply( html, "//*[@id='main-content']/div[4]/span[2]", xmlValue )
temp.time <- gsub( "  ", " 0", unlist(time) )
part <- strsplit( temp.time, split=" ", fixed=TRUE )
timestamp <- part[[1]][4]
timestamp <- strsplit( timestamp, split=":", fixed=TRUE )
hour <- timestamp[[1]][1]
print(hour)
print(temp.time)
name <- paste0('./DATA/', hour, ".txt")
write.table(doc, name, append = TRUE)
}
sapply(data, getdoc)
library(bitops)
library(httr)
library(RCurl)
library(XML)
library(tm)
library(NLP)
library(tmcn)
library(jiebaRD)
library(jiebaR)
rm(list = ls())
library(rvest)
#I aim to analyze the keywords of NBA articles in PTT. Hopefully, the result can show me whether there is a team that is relatively popular in discussion. Also, I want to know what is the direction and keywords in these articles.
#Attain the website of the articles
from <- 3868 # 2018-03-29
to   <- 3876 # 2018-04-01
prefix = "https://www.ptt.cc/bbs/NBA/index"
data <- list()
for( id in c(from:to) )
{
url  <- paste0( prefix, as.character(id), ".html" )
html <- htmlParse( GET(url) )
url.list <- xpathSApply( html, "//div[@class='title']/a[@href]", xmlAttrs )
data <- rbind( data, as.matrix(paste('https://www.ptt.cc', url.list, sep='')) )
#print(paste(id))
#add this line so that I know the for loop has run successfully.
res <- read_html(url)
#print(paste(url))
#by adding this line, I understood the reason why I only output part of the titles from the website.
raw.titles <- res %>% html_nodes("div.title")
nba.article.title <- raw.titles %>% html_text()
}
#I learned the difference between paste() and paste0(), the former can give order to seperate different things in (), and the latter will connect things in () without any blankspace
data <- unlist(data)
#take a loot at the data I successfully got
head(data)
tail(data)
#data
#nba.article.title
#Success
#Attain all the websites.
#Fail
#I want to get the titles of all the articles, but the result is only the last twenty titles.
#Since extracting titles is not the main mission of this project, I focus on the rest of the work.
#Attain the contents of the artickles I look for.
#The difference between this project and the previous assignments is that I want to know the freqeucy of each keywords by time.
#At first, I dealed with extracting all the contents from websites(this part was finished first), and then focuss on sorting the content with hours.And then, I failed. There were lots of errors I couldn't solve, so I put it into the same{}.
#Problem: Perhaps I am processing a lot of data, RStudio isn't functioning well. It took a long time to show me the result after running the programs, which took me more time to work on it.
library(dplyr)
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
time <- xpathSApply( html, "//*[@id='main-content']/div[4]/span[2]", xmlValue )
temp <- gsub( "  ", " 0", unlist(time) )
part <- strsplit( temp, split=" ", fixed=T )
#date <- paste(part[[1]][2], part[[1]][3], part[[1]][5], sep="-")
#date <- paste(part[[1]][2], part[[1]][5], sep="_")
#date <- paste(part[[1]][1], part[[1]][2], sep="_")
timestamp <- part[[1]][4]
timestamp <- strsplit( timestamp, split=":", fixed=T )
hour <- timestamp[[1]][1]
#print(hour)
name <- paste0('./DATA/', hour, ".txt")
write(doc, name, append = TRUE)
}
sapply(data, getdoc)
sapply(data, getdoc)
doc
data
