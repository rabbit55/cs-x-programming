---
title: "Analyzing NBA articles and dicussions in PTT"
author: "christine"
date: "2018/4/21???"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

At First, I tried to analyze the data with time difference, but I couldn?????? combine the content of articles. Therefore, I tried to make the project easier by simply using tfidf to find keywords. I succeeded in this part, but I didn?????? know how to change the form of the data. When I tried to use ggplot2, there was an error message: ggplot2 doesn?????? know how to deal with data of class matrix. In brief, I failed this project. I was not able to visualize the data.

Even though I failed, I still want to note down some reflection and some of my ideas. I aim to analyze NBA articles in PTT. If I were able to add time this factor into account, I would like to combine the matches of NBA. By comparing when the matches end, I may find a connection between the match and the articles content in PTT.

At the end, I only finished tfidf part. To my excitement, I figured out that some popular players, indeed, have higher freqency at the result. Wade, lbj(labron james), Kobe, klay, curry are all famous NBA players, and their names are mentioned significantly more often.

All in all, I???î¨†e spent quite a long time on this project. It is a shame that I didn?????? make it. Hopefully, I will make more achievement in the following projects.




```{r cars}
library(bitops)
library(httr)
library(RCurl)
library(XML)
library(tm)
library(NLP)
library(tmcn)
library(jiebaRD)
library(jiebaR)
library(readtext)
rm(list = ls())
library(rvest)
library(ggplot2)
#I aim to analyze the keywords of NBA articles in PTT. Hopefully, the result can show me whether there is a team that is relatively popular in discussion. Also, I want to know what is the direction and keywords in these articles.

#Attain the website of the articles

from <- 3870 # 2018-03-25
to   <- 3874 # 2018-03-31
prefix = "https://www.ptt.cc/bbs/NBA/index"

data <- list()
for( id in c(from:to) )
{
  url  <- paste0( prefix, as.character(id), ".html" )
  html <- htmlParse( GET(url) )
  url.list <- xpathSApply( html, "//div[@class='title']/a[@href]", xmlAttrs )
  data <- rbind( data, as.matrix(paste('https://www.ptt.cc', url.list, sep='')) )
  #print(paste(id))
  #add this line so that I know the for loop has run successfully.
  res <- read_html(url)
  #print(paste(url))
  #by adding this line, I understood the reason why I only output part of the titles from the website.
  raw.titles <- res %>% html_nodes("div.title")
  nba.article.title <- raw.titles %>% html_text()
}
#I learned the difference between paste() and paste0(), the former can give order to seperate different things in (), and the latter will connect things in () without any blankspace
data <- unlist(data)
#take a loot at the data I successfully got
head(data)

tail(data)

data

#Success
#Attain all the websites.
#Fail
#I want to get the titles of all the articles, but the result is only the last twenty titles.
#Since extracting titles is not the main mission of this project, I focus on the rest of the work.

#Attain the contents of the artickles I look for.
#The difference between this project and the previous assignments is that I want to know the freqeucy of each keywords by time.
#At first, I dealed with extracting all the contents from websites(this part was finished first), and then focuss on sorting the content with hours.And then, I failed. There were lots of errors I couldn't solve, so I put it into the same{}.
#Problem: Perhaps I am processing a lot of data, RStudio isn't functioning well. It took a long time to show me the result after running the programs, which took me more time to work on it.
library(dplyr)

getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
doc  <- xpathSApply( html, "//div[@id='main-content']", xmlValue )
}
content <- sapply(data, getdoc)
head(content)


#Success
#Got all the content, and I also learned the differences between apply, sapply, lapply.
#I am confused how doc works.I understand that I use xpathsApply to gain content, but if I change the name of doc to doc111, it doesnt work.
#Confused, I don't know why there is a lot of blank space in my html. During class, I asked my group mate, ????????????, he presumed that the reason might be that some articles have been deleted.

#Fail
#I couldn't sort the articles in different timing classification. 
#In order to finish this project, I simplified my work without the time factor.
page <- readtext("*.txt", encoding = "big5")
docs <- Corpus(VectorSource(page$text))
docnum = length(page)

mixseg = worker()
Alltoken = list()
Allfreq = list()
#listed all the words and phrases
for( c in 1:docnum )
{
  token = list(jiebaR::segment(docs[[c]]$content, mixseg))
  Alltoken = append(Alltoken, token)
  #use append function to get every words and phrases I need
  freq = list(as.data.frame(table(token)))
  Allfreq = append(Allfreq, freq)
  #use append function to get freqeuncy
}
NewDataFrame = merge(Allfreq[[1]], Allfreq[[2]], by="token", all = TRUE)


#I put the content and comments in to a txt file. However, there was an error message about x$content[[1]]exceed the boundary and the whole running procedure stopps at the forloop. I though this may be the reason that I only imported one flie, and thus the boundary is one, not two. (I am not one hundred percent sure) So, I splitted the txt file into two files, and solved this error message, but another one popped up.
#The second error message I faced is Error in fix.by(by, x, x). I looked up the question in stackoverflow, and knew that when merging data, one can only put two values as imput. 
#I don't really know how to fix this problem, but I realized that when I imported the txt files, I can't read them from RStudio. So, I fixed this probelm (Still confused, will ask Teachers or Teaching assistance on thursday) and Rmarkdown ran smoothly and showed the html file.

library(NLP)
library(tm)
library(stats)
library(proxy)
library(dplyr)
library(readtext)
library(jiebaRD)
library(jiebaR)
library(slam)
library(Matrix)
library(tidytext)


#tfidf
rawData = readtext("*.txt")
docs = Corpus(VectorSource(rawData$text))
# clean the data, which I have learned when plotting wordcloud
toSpace <- content_transformer(function(x, pattern) {
  return (gsub(pattern, " ", x))
})
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
#docs <- tm_map(docs, toSpace, "[a-zA-Z]")
#since during the discussion, some player's mames may be in English, I want to know what players are discussed a lot in these articles.

# cut the words 
keywords = read.csv("keywords.csv")
mixseg = worker()
keys = as.matrix(keywords)
new_user_word(mixseg, keys)

jieba_tokenizer = function(d){
  unlist(segment(d[[1]], mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))

d.corpus <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(d.corpus)
print( tf <- as.matrix(tdm) )
DF <- tidy(tf)

# use the formula I learned to compute tf-idf
N = tdm$ncol
tf <- apply(tdm, 2, sum)
idfCal <- function(word_doc)
{ 
  log2( N / nnzero(word_doc) ) 
}
idf <- apply(tdm, 1, idfCal)

doc.tfidf <- as.matrix(tdm)
for(x in 1:nrow(tdm))
{
  for(y in 1:ncol(tdm))
  {
    doc.tfidf[x,y] <- (doc.tfidf[x,y] / tf[y]) * idf[x]
  }
}

findZeroId = as.matrix(apply(doc.tfidf, 1, sum))
tfidfnn <- doc.tfidf[-which(findZeroId == 0),]

#write.csv(tfidfnn, "show.csv")

#Here I tried to see the first six keywords in a table. so I installed the packages library Matrix, but kable(), this function is not found. Eventually, I typed kable() r lilbrary, and found the library(knitr). Seems that my ability of finding solutions on my own is getting better and better. 
library(knitr)
kable(head(tfidfnn))
kable(tail(tfidfnn))
```


```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
